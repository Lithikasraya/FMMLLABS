{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lithikasraya/FMMLLABS/blob/main/FMML_M1L4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU9LZjQxiQT2"
      },
      "source": [
        "# Transforming data using linear algebra\n",
        "\n",
        "FMML Module 1, Lab 4\n",
        "\n",
        "Matrix transformations are at the heart of many machine learning algorithms. In this lab, we'll visualize the effect of some simple transformations on a unit square and then visualize it using the MNIST dataset. We also see what data normalization means and how it can help in improving the accuracy of machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBe4ZA32UTbv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7mqG-dafVpya"
      },
      "outputs": [],
      "source": [
        "# You don't need to understand these functions\n",
        "\n",
        "\n",
        "def plotGrid(transform, unit, linestyle=\":\", fig=None, ax=None):\n",
        "    lim1 = -100\n",
        "    lim2 = 100\n",
        "\n",
        "    def mat2xy(start, end):\n",
        "        if len(start.shape) == 1:\n",
        "            start = np.expand_dims(start, 0)\n",
        "            end = np.expand_dims(end, 0)\n",
        "        nan = np.ones(len(start)) * np.nan\n",
        "        x = np.stack((start[:, 0], end[:, 0], nan)).T.reshape(-1)\n",
        "        y = np.stack((start[:, 1], end[:, 1], nan)).T.reshape(-1)\n",
        "        return x, y\n",
        "\n",
        "    def parallellines(axis, addend, lines, unit):\n",
        "        addend = np.repeat(np.expand_dims(addend, 0), lines * 2, 0)\n",
        "        unit = np.expand_dims(np.arange(-lines, lines) * unit, 1)\n",
        "        unit = unit - lines\n",
        "        addend = addend * unit\n",
        "        lines = np.expand_dims(axis, 0) + addend\n",
        "        return np.concatenate((lines, lines * -1))\n",
        "\n",
        "    if fig is None:\n",
        "        fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    transform = transform.astype(float)\n",
        "    xaxis = transform[0]\n",
        "    yaxis = transform[1]\n",
        "\n",
        "    # plot lines parallel to the x axis\n",
        "    lines1 = parallellines(xaxis * lim1, yaxis, 100, unit)\n",
        "    lines2 = parallellines(xaxis * lim2, yaxis, 100, unit)\n",
        "    x, y = mat2xy(lines1, lines2)\n",
        "    plt.plot(x, y, linestyle + \"k\", linewidth=0.5)\n",
        "    # plot x axis\n",
        "    x, y = mat2xy(xaxis * lim1, xaxis * lim2)\n",
        "    plt.plot(x, y, linestyle, color=\"#440077\")\n",
        "\n",
        "    # plot  lines parallel to the y axis\n",
        "    lines1 = parallellines(yaxis * lim1, xaxis, 100, unit)\n",
        "    lines2 = parallellines(yaxis * lim2, xaxis, 100, unit)\n",
        "    x, y = mat2xy(lines1, lines2)\n",
        "    plt.plot(x, y, linestyle + \"k\", linewidth=0.5)\n",
        "    # plot y axis\n",
        "    x, y = mat2xy(yaxis * lim1, yaxis * lim2)\n",
        "    plt.plot(x, y, linestyle, color=\"#aa5500\")\n",
        "\n",
        "    return fig, ax\n",
        "\n",
        "\n",
        "def plotData(X, y, xlabel=\"hole\", ylabel=\"bound\", fig=None, ax=None):\n",
        "    if fig is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    for ii in range(nclasses):\n",
        "        plt.scatter(X[y == ii, 0], X[y == ii, 1])\n",
        "    plt.legend([str(i) for i in range(nclasses)])\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    lim2 = X.max()\n",
        "    lim1 = X.min()\n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d5dmEXxaktp"
      },
      "source": [
        "## Matrix transformations on data\n",
        "\n",
        "Note: This lab involves a lot of matrix operations. If you are not familiar with them, please go through the resources given in class before proceeding. You can also review Khan Academy's excellent linear algebra [resources](https://www.khanacademy.org/math/linear-algebra/matrix-transformations).\n",
        "\n",
        "A 2D coordinate system is defined by its basis vectors, i and j. Any point in this 2D space can be represented as a linear combination of these basis vectors. For example, the point (a,b) can be represented as:\n",
        "\n",
        "$$\\begin{equation}\n",
        "\\left\\{  \\begin{aligned}a \\\\ b \\end{aligned} \\right\\} = a\\left\\{  \\begin{aligned}1 \\\\ 0 \\end{aligned} \\right\\} + b\\left\\{  \\begin{aligned}0 \\\\ 1 \\end{aligned} \\right\\} = a\\hat{i} + b\\hat{j}\n",
        "\\end{equation}$$\n",
        "\n",
        "A matrix can be used to perform a linear transformation on the basis vectors. The new basis vectors $\\hat{i}$ and $\\hat{j}$ are given by the product of the matrix and the basis vectors of the standard coordinate system.\n",
        "\n",
        "In the standard coordinate system (Let us call it T0), the basis vectors are\n",
        "\n",
        "$$\\begin{equation}\n",
        "i = \\left\\{  \\begin{aligned}1 \\\\ 0 \\end{aligned} \\right\\}\n",
        "\\end{equation}$$\n",
        "and\n",
        "$$\\begin{equation} j = \\left\\{ \\begin{aligned} 0 \\\\ 1\\end{aligned} \\right\\} \\end{equation}$$\n",
        "\n",
        "We can use any two vectors as basis vectors for a new coordinate system as long as they are not colinear. For example, let us call this new coordinate system T1:\n",
        "\n",
        "$$\\begin{equation}\n",
        "i = \\left\\{  \\begin{aligned}1 \\\\ -1 \\end{aligned} \\right\\}\n",
        "\\end{equation}$$\n",
        "and\n",
        "$$\\begin{equation} j = \\left\\{ \\begin{aligned} 0 \\\\ 2 \\end{aligned} \\right\\} \\end{equation}$$\n",
        "\n",
        "Suppose we have a point [a,b] in the T1 coordinate system. Its representation in the standard system T0 can be obtained by the following matrix multiplication:\n",
        "\n",
        "$$ \\begin{equation}\n",
        "\\left\\{  \\begin{aligned}a' \\\\ b' \\end{aligned} \\right\\} =\n",
        "\\left\\{  \\begin{aligned}&1 & 0 \\\\ -&1 & 2 \\end{aligned} \\right\\}\n",
        "\\left\\{  \\begin{aligned}a \\\\ b \\end{aligned} \\right\\}\n",
        "\\end{equation}$$\n",
        "where the columns of the matrix are the basis vectors of T1.\n",
        "\n",
        "\n",
        "Let us see this in action:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8SCFKqfbM-l"
      },
      "outputs": [],
      "source": [
        "T0 = np.array([[1, 0], [0, 1]])\n",
        "T1 = np.array([[1, 0], [-1, 2]])\n",
        "\n",
        "data1 = np.array([5, 4])  # the data in T1 coordinate system\n",
        "data0 = np.matmul(T1, data1)  # the data in T0 coordinate system\n",
        "\n",
        "print(\"Data in T0 = \", data0)\n",
        "print(\"Data in T1 = \", data1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIfEWZ1RQeve"
      },
      "source": [
        "We can visualize this below. T0 is shown with dotted lines and T1 is shown with solid lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSuTaeSDQoYK"
      },
      "outputs": [],
      "source": [
        "fig, ax = plotGrid(T1.T, 1, \"-\")\n",
        "plotGrid(T0.T, 1, fig=fig, ax=ax)\n",
        "\n",
        "plt.scatter(data0[0], data0[1])\n",
        "ax.set_xlim(-10, 10)\n",
        "ax.set_ylim(-10, 10)\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUb3oYWIgdya"
      },
      "source": [
        "Look at the coordinates of the blue dot. In T0 (dotted lines), the position is [5,3] where it is [5,4] in T1. Feel free to experiment with different data points and coordinate systems.\n",
        "\n",
        "Remember that we can achieve the same thing by post-multiplying the transpose of the transformation matrix to the data. This will come in handy when transforming multiple data points at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SD4pqOMndf-4"
      },
      "outputs": [],
      "source": [
        "data0_a = np.matmul(T1, data1)\n",
        "data0_b = np.matmul(data1, T1.T)\n",
        "print(data0_a)\n",
        "print(data0_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-Kui3dSESPm"
      },
      "source": [
        "Why is transforming data useful? Data transformations cause the distance between data points to change. This will affect distance-based algorithms such as nearest neighbour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE0NbpYIC9ou"
      },
      "outputs": [],
      "source": [
        "# let us define 3 points in T1\n",
        "A1 = np.array([3, 3])\n",
        "B1 = np.array([2, -5])\n",
        "C1 = np.array([1, -1])\n",
        "\n",
        "# the corresponding points in T0:\n",
        "A0 = np.matmul(T1, A1)\n",
        "B0 = np.matmul(T1, B1)\n",
        "C0 = np.matmul(T1, C1)\n",
        "\n",
        "\n",
        "def dist(a, b):\n",
        "    # function to calculate Euclidean distance between two points\n",
        "    diff = a - b\n",
        "    sq = diff * diff\n",
        "    return np.sqrt(sq.sum())\n",
        "\n",
        "\n",
        "# distance between the points in T1\n",
        "print(\"Distance between A and B in T1 = \", dist(A1, B1))\n",
        "print(\"Distance between B and C in T1 = \", dist(B1, C1))\n",
        "print(\"Distance between A and C in T1 = \", dist(A1, C1))\n",
        "\n",
        "print(\"\")\n",
        "# distnace between the points in T0\n",
        "print(\"Distance between A and B in T0 = \", dist(A0, B0))\n",
        "print(\"Distance between B and C in T0 = \", dist(B0, C0))\n",
        "print(\"Distance between A and C in T0 = \", dist(A0, C0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE95JMniUtDm"
      },
      "source": [
        "We see that in T1, B and C are the closest whereas in T0, A and C are the closest. These kinds of changes will affect the predictions returned by the nearest neighbour algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFbpTSBdV29C"
      },
      "source": [
        "## Transformations on MNIST\n",
        "\n",
        "Let us experiment with a subset of the MNIST dataset. We will extract two features from the database for our experiment. We will then transform the data using a transformation matrix and visualize the data in the new coordinate system. We will also see how normalization can help in improving the accuracy of the model. We will reuse previous labs code for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "22ayl0sViF5-"
      },
      "outputs": [],
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and a query point\n",
        "    and returns the predicted label for the query point using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    query: numpy array of shape (d,) where d is the number of features\n",
        "\n",
        "    returns: the predicted label for the query point which is the label of the training data which is closest to the query point\n",
        "    \"\"\"\n",
        "    diff = (\n",
        "        traindata - query\n",
        "    )  # find the difference between features. Numpy automatically takes care of the size here\n",
        "    sq = diff * diff  # square the differences\n",
        "    dist = sq.sum(1)  # add up the squares\n",
        "    label = trainlabel[np.argmin(dist)]\n",
        "    return label\n",
        "\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and test data\n",
        "    and returns the predicted labels for the test data using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    testdata: numpy array of shape (m,d) where m is the number of test samples and d is the number of features\n",
        "\n",
        "    returns: the predicted labels for the test data which is the label of the training data which is closest to each test point\n",
        "    \"\"\"\n",
        "    predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "    return predlabel\n",
        "\n",
        "\n",
        "def Accuracy(gtlabel, predlabel):\n",
        "    \"\"\"\n",
        "    This function takes in the ground-truth labels and predicted labels\n",
        "    and returns the accuracy of the classifier\n",
        "\n",
        "    gtlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    predlabel: numpy array of shape (n,) where n is the number of samples\n",
        "\n",
        "    returns: the accuracy of the classifier which is the number of correct predictions divided by the total number of predictions\n",
        "    \"\"\"\n",
        "    assert len(gtlabel) == len(\n",
        "        predlabel\n",
        "    ), \"Length of the ground-truth labels and predicted labels should be the same\"\n",
        "    correct = (\n",
        "        gtlabel == predlabel\n",
        "    ).sum()  # count the number of times the groundtruth label is equal to the predicted label.\n",
        "    return correct / len(gtlabel)\n",
        "\n",
        "\n",
        "def cumArray(img):\n",
        "    img2 = img.copy()\n",
        "    for ii in range(1, img2.shape[1]):\n",
        "        # for every row, add up all the rows above it.\n",
        "        img2[ii, :] = img2[ii, :] + img2[ii - 1, :]\n",
        "    img2 = img2 > 0\n",
        "    return img2\n",
        "\n",
        "\n",
        "def getHolePixels(img):\n",
        "    \"\"\"\n",
        "    This function takes in a binary image and returns the pixels that are holes in the image\n",
        "\n",
        "    img: numpy array of shape (n,m) where n is the height of the image and m is the width of the image\n",
        "\n",
        "    returns: a binary image of the same shape as the input image where the holes are filled in\n",
        "    \"\"\"\n",
        "    im1 = cumArray(img)\n",
        "    # rotate and cumulate it again for differnt direction\n",
        "    im2 = np.rot90(cumArray(np.rot90(img)), 3)\n",
        "    im3 = np.rot90(cumArray(np.rot90(img, 2)), 2)\n",
        "    im4 = np.rot90(cumArray(np.rot90(img, 3)), 1)\n",
        "    # this will create a binary image with all the holes filled in.\n",
        "    hull = im1 & im2 & im3 & im4\n",
        "    # remove the original digit to leave behind the holes\n",
        "    hole = hull & ~(img > 0)\n",
        "    return hole\n",
        "\n",
        "\n",
        "def getHullPixels(img):\n",
        "    \"\"\"\n",
        "    This function takes in a binary image and returns the pixels that are the convex hull of the image\n",
        "\n",
        "    img: numpy array of shape (n,m) where n is the height of the image and m is the width of the image\n",
        "\n",
        "    returns: a binary image of the same shape as the input image where the convex hull is filled in\n",
        "    \"\"\"\n",
        "    im1 = cumArray(img)\n",
        "    # rotate and cumulate it again for differnt direction\n",
        "    im2 = np.rot90(cumArray(np.rot90(img)), 3)\n",
        "    im3 = np.rot90(cumArray(np.rot90(img, 2)), 2)\n",
        "    im4 = np.rot90(cumArray(np.rot90(img, 3)), 1)\n",
        "    # this will create a binary image with all the holes filled in.\n",
        "    hull = im1 & im2 & im3 & im4\n",
        "    return hull\n",
        "\n",
        "\n",
        "def minus(a, b):\n",
        "    \"\"\"\n",
        "    This function takes in two binary images and returns the difference between the two images\n",
        "    \"\"\"\n",
        "    return a & ~b\n",
        "\n",
        "\n",
        "def getBoundaryPixels(img):\n",
        "    \"\"\"\n",
        "    This function takes in a binary image and returns the pixels that are the boundary of the image\n",
        "\n",
        "    img: numpy array of shape (n,m) where n is the height of the image and m is the width of the image\n",
        "\n",
        "    returns: a binary image of the same shape as the input image where the boundary is filled in\n",
        "    \"\"\"\n",
        "    img = img.copy() > 0  # binarize the image\n",
        "    rshift = np.roll(img, 1, 1)\n",
        "    lshift = np.roll(img, -1, 1)\n",
        "    ushift = np.roll(img, -1, 0)\n",
        "    dshift = np.roll(img, 1, 0)\n",
        "    boundary = (\n",
        "        minus(img, rshift)\n",
        "        | minus(img, lshift)\n",
        "        | minus(img, ushift)\n",
        "        | minus(img, dshift)\n",
        "    )\n",
        "    return boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHz5BVmLUjzb"
      },
      "outputs": [],
      "source": [
        "# loading the dataset\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "train_X = train_X / 255\n",
        "test_X = test_X / 255\n",
        "\n",
        "nclasses = 4\n",
        "\n",
        "# get only for the first 4 classes\n",
        "train_X = train_X[train_y < nclasses]\n",
        "train_y = train_y[train_y < nclasses]\n",
        "test_X = test_X[test_y < nclasses]\n",
        "test_y = test_y[test_y < nclasses]\n",
        "\n",
        "# We are only taking a subset of the training set\n",
        "train_X = train_X[::100].copy()\n",
        "train_y = train_y[::100].copy()  # do the same to the labels\n",
        "\n",
        "# taking a subset of the test set. This code takes every 500th sample\n",
        "test_X = test_X[::100].copy()\n",
        "test_y = test_y[::100].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvavjmfvH515"
      },
      "outputs": [],
      "source": [
        "# feature extraction\n",
        "train_hole = np.array([getHolePixels(i).sum() for i in train_X])\n",
        "test_hole = np.array([getHolePixels(i).sum() for i in test_X])\n",
        "train_bound = np.array([getBoundaryPixels(i).sum() for i in train_X])\n",
        "test_bound = np.array([getBoundaryPixels(i).sum() for i in test_X])\n",
        "# train_hull = np.array([getHullPixels(i).sum() for i in train_X])\n",
        "# test_hull = np.array([getHullPixels(i).sum() for i in test_X])\n",
        "# train_sum = np.sum(train_X, (1, 2)) / (28 * 28)\n",
        "# test_sum = np.sum(test_X, (1, 2)) / (28 * 28)\n",
        "\n",
        "# create the train and test set by combining the appropriate features\n",
        "train_feats = np.vstack(\n",
        "    (train_hole, train_bound)).transpose()\n",
        "test_feats = np.vstack(\n",
        "    (test_hole, test_bound)).transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7qGVrlnQCUy"
      },
      "source": [
        "Let us plot the samples and see what they look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saRzHfi9QAGd"
      },
      "outputs": [],
      "source": [
        "# fix limits of x and y axis so that we can see what is going on\n",
        "xlim = [-100, 300]\n",
        "ylim = [-100, 300]\n",
        "fig, ax = plotData(train_feats, train_y)\n",
        "ax.set_xlim(xlim)\n",
        "ax.set_ylim(ylim)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCb4DikQP1ck"
      },
      "source": [
        "Check the baseline accuracy on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxVr6bd9PzlI"
      },
      "outputs": [],
      "source": [
        "test_pred = NN(train_feats, train_y, test_feats)\n",
        "acc = Accuracy(test_y, test_pred)\n",
        "print(\"Baseline accuracy:\", acc*100, \"%\", \"for\", nclasses, \"classes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl8Noo8pZRek"
      },
      "source": [
        "Let us try transforming the features and checking their accuracy. The intuition to using the transformation matrix is to find the basis vectors of the dataset and transform the data to a new coordinate system where the basis vectors are orthogonal. This will help in reducing the redundancy in the data and improve the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dUPWRsEZKwo"
      },
      "outputs": [],
      "source": [
        "transform = np.array([[0.5, -0.5], [0, 2.5]])\n",
        "print(transform)\n",
        "\n",
        "train_feats_t = np.matmul(train_feats, transform)\n",
        "# whatever transform we are applying to the training set should be applied to the test set also\n",
        "test_feats_t = np.matmul(test_feats, transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRH4VckHZaWv"
      },
      "outputs": [],
      "source": [
        "fig, ax = plotData(train_feats_t, train_y)\n",
        "ax.set_xlim(xlim)\n",
        "ax.set_ylim(ylim)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9fknczfZdYF"
      },
      "outputs": [],
      "source": [
        "test_pred = NN(train_feats_t, train_y, test_feats_t)\n",
        "acc = Accuracy(test_y, test_pred)\n",
        "print(\"Baseline accuracy:\", acc*100, \"%\", \"for\", nclasses, \"classes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFBQlAnNZ3on"
      },
      "source": [
        "## Questions:\n",
        "1. Experiment with different transformation matrices and check the accuracy\n",
        "2. Will the same transform used for these two features also work for other features?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXCnbDsVH516"
      },
      "source": [
        "> Exercise: Is it possible that adding all 4 features at a time is not the best strategy? Can you think of a better combination of features that can help in improving the accuracy of the model? Maybe you can try adding 2 features at a time and see if that helps."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1. The effect of different transformation matrices on accuracy in machine learning is a crucial aspect to consider when working with data.\n",
        "\n",
        "In machine learning, transformation matrices are used to transform the input data into a more suitable form for the algorithm to process. This can include operations such as feature scaling, normalization, and dimensionality reduction.\n"
      ],
      "metadata": {
        "id": "wuReD4kik4Pa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a sample dataset\n",
        "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# Define a transformation matrix\n",
        "A = np.array([[2, 1], [1, 3]])\n",
        "\n",
        "# Apply the transformation\n",
        "X_transformed = np.dot(X, A)\n",
        "\n",
        "print(X_transformed)"
      ],
      "metadata": {
        "id": "sLUVPts-lC3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[[ 4  7]\n",
        " [10 15]\n",
        " [16 23]]\n",
        "In this example, the transformation matrix A is applied to the input data X using matrix multiplication. The resulting transformed data X_transformed can then be used as input to a machine learning algorithm.\n",
        "\n",
        "Now, let's discuss how different transformation matrices can affect the accuracy of a machine learning model."
      ],
      "metadata": {
        "id": "r9g1svgblLw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T0_3ZyVQlSfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Effect of Transformation Matrices on Accuracy\n",
        "\n",
        "The choice of transformation matrix can significantly impact the accuracy of a machine learning model. Here are some ways in which different transformation matrices can affect accuracy:\n",
        "\n",
        "Feature scaling: If the features in the input data have different scales, a transformation matrix can be used to scale them to a common range. This can improve the accuracy of the model by preventing features with large ranges from dominating the model.\n",
        "Dimensionality reduction: If the input data has a high dimensionality, a transformation matrix can be used to reduce the dimensionality while retaining the most important features. This can improve the accuracy of the model by reducing the risk of overfitting.\n",
        "Noise reduction: A transformation matrix can be used to reduce noise in the input data, which can improve the accuracy of the model by reducing the impact of noisy data points.\n",
        "To experiment with different transformation matrices and check the accuracy, you can follow these steps:\n",
        "\n",
        "Split the data: Split the input data into training and testing sets.\n",
        "Apply transformation matrices:Apply different transformation matrices to the training data and evaluate the accuracy of the model on the testing data. 3.Compare results: Compare the accuracy of the model with different transformation matrices and select the one that results in the highest accuracy."
      ],
      "metadata": {
        "id": "7HKllaYGlcC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define a sample dataset\n",
        "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "y = np.array([0, 0, 1])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define different transformation matrices\n",
        "A1 = np.array([[2, 1], [1, 3]])\n",
        "A2 = np.array([[1, 2], [3, 1]])\n",
        "A3 = np.array([[3, 1], [1, 2]])\n",
        "\n",
        "# Apply the transformation matrices\n",
        "X_train_transformed1 = np.dot(X_train, A1)\n",
        "X_train_transformed2 = np.dot(X_train, A2)\n",
        "X_train_transformed3 = np.dot(X_train, A3)\n",
        "\n",
        "# Train and evaluate the model with different transformation matrices\n",
        "model1 = LogisticRegression()\n",
        "model1.fit(X_train_transformed1, y_train)\n",
        "y_pred1 = model1.predict(X_test)\n",
        "accuracy1 = accuracy_score(y_test, y_pred1)\n",
        "\n",
        "model2 = LogisticRegression()\n",
        "model2.fit(X_train_transformed2, y_train)\n",
        "y_pred2 = model2.predict(X_test)\n",
        "accuracy2 = accuracy_score(y_test, y_pred2)\n",
        "\n",
        "model3 = LogisticRegression()\n",
        "model3.fit(X_train_transformed3, y_train)\n",
        "y_pred3 = model3.predict(X_test)\n",
        "accuracy3 = accuracy_score(y_test, y_pred3)\n",
        "\n",
        "# Compare the results\n",
        "print(\"Accuracy with transformation matrix A1:\", accuracy1)\n",
        "print(\"Accuracy with transformation matrix A2:\", accuracy2)\n",
        "print(\"Accuracy with transformation matrix A3:\", accuracy3)"
      ],
      "metadata": {
        "id": "_i7T3emvl5EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  It depends on the model you are using and the characteristics of your data. In some cases, using the same transformation for multiple features can be beneficial, while in other cases, it may not be the best approach.\n",
        "\n",
        "For example, if you have features that are highly correlated, using the same transformation for both features can help to reduce the correlation and improve the model's performance.\n",
        "\n",
        "On the other hand, if you have features with different distributions or scales, using the same transformation for all features may not be effective. In such cases, it may be better to use different transformations for different features, or to use a transformation that is robust to different distributions, such as the RobustScaler or QuantileTransformer.\n",
        "\n",
        "Ultimately, the choice of transformation will depend on the specific characteristics of your data and the goals of your model. It's always a good idea to experiment with different transformations and evaluate their impact on your model's performance.\n",
        "\n",
        "Here are some common feature transformations that can be used:\n",
        "\n",
        "StandardScaler: Standardizes features by removing the mean and scaling to unit variance.\n",
        "MinMaxScaler: Scales features to a common range, usually between 0 and 1.\n",
        "MaxAbsScaler: Scales features to a common range, usually between -1 and 1.\n",
        "RobustScaler:Scales features using the interquartile range (IQR) instead of the standard deviation.\n",
        "QuantileTransformer:Scales features to a common distribution, usually a uniform or normal distribution.\n",
        "PowerTransformer: Scales features using a power transformation, such as a square root or logarithmic transformation.\n",
        "LogTransformer: Scales features using a logarithmic transformation.\n",
        "Normalizer: Scales features to a common norm, usually the L1 or L2 norm."
      ],
      "metadata": {
        "id": "49yh2jj5mEAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, QuantileTransformer, PowerTransformer, Normalizer\n",
        "\n",
        "# Define a sample dataset\n",
        "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# Apply different transformations\n",
        "scaler1 = StandardScaler()\n",
        "X_scaled1 = scaler1.fit_transform(X)\n",
        "\n",
        "scaler2 = MinMaxScaler()\n",
        "X_scaled2 = scaler2.fit_transform(X)\n",
        "\n",
        "scaler3 = MaxAbsScaler()\n",
        "X_scaled3 = scaler3.fit_transform(X)\n",
        "\n",
        "scaler4 = RobustScaler()\n",
        "X_scaled4 = scaler4.fit_transform(X)\n",
        "\n",
        "scaler5 = QuantileTransformer()\n",
        "X_scaled5 = scaler5.fit_transform(X)\n",
        "\n",
        "scaler6 = PowerTransformer()\n",
        "X_scaled6 = scaler6.fit_transform(X)\n",
        "\n",
        "scaler7 = Normalizer()\n",
        "X_scaled7 = scaler7.fit_transform(X)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import itertools\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "\n",
        "# Example data\n",
        "data = pd.read_csv('data.csv')\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "\n",
        "# Model to evaluate\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Function to evaluate combinations of features\n",
        "def evaluate_combinations(X, y, feature_combinations):\n",
        "    results = []\n",
        "    for combination in feature_combinations:\n",
        "        X_subset = X[list(combination)]\n",
        "        scores = cross_val_score(model, X_subset, y, cv=5, scoring='accuracy')\n",
        "        results.append((combination, scores.mean()))\n",
        "    return results\n",
        "\n",
        "# Generate combinations of features\n",
        "features = X.columns\n",
        "combinations = list(itertools.combinations(features, 2))\n",
        "\n",
        "# Evaluate feature combinations\n",
        "results = evaluate_combinations(X, y, combinations)\n",
        "\n",
        "# Sort and print results\n",
        "results.sort(key=lambda x: x[1], reverse=True)\n",
        "for combination, score in results:\n",
        "    print(f'Combination: {combination}, Accuracy: {score:.4f}')"
      ],
      "metadata": {
        "id": "sbS081YCmL_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36TOA47xak20"
      },
      "source": [
        "# Data normalization\n",
        "\n",
        "Sometimes the features of our data have vastly different scales. This will cause the learning algorithm to give more importance to certain features, reducing its performance. Data normalization is a method in which we transform the features so that they have similar scales.\n",
        "\n",
        "Three commonly used feature scaling techniques are rescaling, mean normalization and z-score normalization. Here, we will talk about the simplest one: rescaling.\n",
        "\n",
        "$$\\begin{equation}\n",
        "x' = \\frac {x -min(x)} { max(x) - min(x)}\n",
        "\\end{equation}$$\n",
        "\n",
        "\n",
        "\n",
        "For more information, see [this page](https://towardsdatascience.com/data-normalization-in-machine-learning-395fdec69d02)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni19QKDLZzeo"
      },
      "outputs": [],
      "source": [
        "def rescale(data):\n",
        "    return (data - data.min()) / (data.max() - data.min())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k83ZMtKeZrQ"
      },
      "source": [
        "We have to apply the rescaling to each feature individually. Also remember to apply the same transform we are using on the train set to the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JLOPwhvehpR"
      },
      "outputs": [],
      "source": [
        "train_feats_rescaled_x = rescale(train_feats[:, 0])\n",
        "train_feats_rescaled_y = rescale(train_feats[:, 1])\n",
        "train_feats_rescaled = np.stack((train_feats_rescaled_x, train_feats_rescaled_y), 1)\n",
        "\n",
        "test_feats_rescaled_x = rescale(test_feats[:, 0])\n",
        "test_feats_rescaled_y = rescale(test_feats[:, 1])\n",
        "test_feats_rescaled = np.stack((test_feats_rescaled_x, test_feats_rescaled_y), 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaZVy9vxfKwX"
      },
      "source": [
        "Let us plot the rescaled features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXyatpwZevOH"
      },
      "outputs": [],
      "source": [
        "fig, ax = plotData(train_feats_rescaled, train_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjBgxE6zglsL"
      },
      "source": [
        "This type of rescaling makes all the features between 0 and 1.\n",
        "\n",
        "Let us calculate the accuracy obtained by this transform:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKQnsj-KgNZc"
      },
      "outputs": [],
      "source": [
        "test_pred = NN(train_feats_rescaled, train_y, test_feats_rescaled)\n",
        "acc = Accuracy(test_y, test_pred)\n",
        "print(\"Accuracy after transform:\", acc*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qYX90Gqg-jO"
      },
      "source": [
        "All 2D linear transformations can be repreented by a transformation matrix. So what is the matrix associated with the rescaling function? Actually, we cannot represent rescaling with a matrix multiplication, because it is not a linear transform. Rescaling involves shifting the origin of the data, which is not allowed under linear transformations.\n",
        "\n",
        "We can represent rescaling as a matrix multiplication followed by a vector addition. Let our first feature vector be called X and second feature vector be called Y. Suppose we want to rescale a data point [a,b]\n",
        "\n",
        "$$ \\begin{equation}\n",
        " \\left\\{  \\begin{aligned}a' \\\\ b' \\end{aligned} \\right\\} =\n",
        " \\left\\{  \\begin{aligned} \\frac{a - min(X)}{max(X) - min(X)} \\\\ \\frac{b - min(Y)}{max(Y) - min(Y)} \\end{aligned} \\right\\} =\n",
        " \\left\\{  \\begin{aligned}&\\frac{1}{max(X)-min(X)} &0\\\\ &0 &\\frac{1}{max(Y)-min(Y)} \\end{aligned}\n",
        " \\right\\}\\left\\{  \\begin{aligned}a \\\\ b \\end{aligned} \\right\\} +\n",
        " \\left\\{  \\begin{aligned} \\frac{ -min(X)}{max(X) - min(X)} \\\\ \\frac{-min(Y)}{max(Y) - min(Y)} \\end{aligned} \\right\\}\n",
        "\\end{equation}$$\n",
        "\n",
        "You can verify this yourself if you wish, though it is not necessary.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}